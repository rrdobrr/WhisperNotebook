
import os
import subprocess
import torch
from pydub import AudioSegment
from faster_whisper import WhisperModel
from datetime import timedelta
from tqdm import tqdm

# === –ù–ê–°–¢–†–û–ô–ö–ò ===
videos_folder = "videos"
audio_temp = "temp_audio.wav"

# === –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—É–¥–∏–æ ===
def extract_audio(video_path, out_wav="temp_audio.wav", sr=16000):
    if not os.path.isfile(video_path):
        raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {video_path}")
    command = [
        "ffmpeg", "-y",
        "-i", video_path,
        "-ac", "1",
        "-ar", str(sr),
        "-vn",
        out_wav
    ]
    subprocess.run(command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    return out_wav

# === –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ ===
def format_timestamp(seconds):
    td = timedelta(seconds=seconds)
    result = str(td)
    if '.' not in result:
        result += '.000000'
    result = result[:12].replace('.', ',')
    if "," not in result:
        result += ",000"
    elif len(result.split(",")[1]) < 3:
        result += "0" * (3 - len(result.split(",")[1]))
    return result.zfill(12)

# === –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–¥ ===

print("üîß –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å large-v2 –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏...")
model = WhisperModel(
    "large-v2",
    device="cpu",
    compute_type="int8"
)

# –ü–æ–∏—Å–∫ –≤—Å–µ—Ö –≤–∏–¥–µ–æ—Ñ–∞–π–ª–æ–≤
video_files = []
for root, _, files in os.walk(videos_folder):
    for file in files:
        if file.lower().endswith((".mp4", ".mkv", ".mov", ".avi")):
            video_files.append(os.path.join(root, file))

if not video_files:
    print("‚ùå –ù–µ—Ç –≤–∏–¥–µ–æ—Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ 'videos/'")
    exit(1)

print(f"üé¨ –ù–∞–π–¥–µ–Ω–æ {len(video_files)} –≤–∏–¥–µ–æ—Ñ–∞–π–ª–æ–≤. –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É...")

for video_path in video_files:
    base_name = os.path.splitext(os.path.basename(video_path))[0]
    output_srt = os.path.join(os.path.dirname(video_path), f"{base_name}.srt")

    print(f"\nüé• –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª: {video_path}")
    extract_audio(video_path, audio_temp)

    segments, _ = model.transcribe(audio_temp, language="ru", beam_size=1)

    subs = []
    segment_id = 1

    for segment in segments:
        start = format_timestamp(segment.start)
        end = format_timestamp(segment.end)
        text = segment.text.strip()

        print(f"[{start} --> {end}] {text}")

        subs.append((segment_id, start, end, text))
        segment_id += 1

    with open(output_srt, "w", encoding="utf-8") as f:
        for i, start, end, text in subs:
            f.write(f"{i}\n{start} --> {end}\n{text}\n\n")

    print(f"‚úÖ –°—É–±—Ç–∏—Ç—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_srt}")

# –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞
if os.path.exists(audio_temp):
    os.remove(audio_temp)

print("\nüèÅ –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")



